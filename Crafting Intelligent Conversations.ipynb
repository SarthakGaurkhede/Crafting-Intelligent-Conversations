{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intents': [{'tag': 'greeting',\n",
       "   'patterns': ['Hi', 'Hey', 'Is anyone there?', 'Hello', 'Hay'],\n",
       "   'responses': ['Hello', 'Hi', 'Hi there']},\n",
       "  {'tag': 'goodbye',\n",
       "   'patterns': ['Bye', 'See you later', 'Goodbye'],\n",
       "   'responses': ['See you later', 'Have a nice day', 'Bye! Come back again']},\n",
       "  {'tag': 'thanks',\n",
       "   'patterns': ['Thanks',\n",
       "    'Thank you',\n",
       "    \"That's helpful\",\n",
       "    'Thanks for the help'],\n",
       "   'responses': ['Happy to help!',\n",
       "    'Any time!',\n",
       "    'My pleasure',\n",
       "    \"You're most welcome!\"]},\n",
       "  {'tag': 'about',\n",
       "   'patterns': ['Who are you?', 'What are you?', 'Who you are?'],\n",
       "   'responses': ['I.m Joana, your bot assistant',\n",
       "    \"I'm Joana, an Artificial Intelligent bot\"]},\n",
       "  {'tag': 'name',\n",
       "   'patterns': ['what is your name',\n",
       "    'what should I call you',\n",
       "    'whats your name?'],\n",
       "   'responses': ['You can call me Joana.',\n",
       "    \"I'm Joana!\",\n",
       "    'Just call me as Joana']},\n",
       "  {'tag': 'help',\n",
       "   'patterns': ['Could you help me?',\n",
       "    'give me a hand please',\n",
       "    'Can you help?',\n",
       "    'What can you do for me?',\n",
       "    'I need a support',\n",
       "    'I need a help',\n",
       "    'support me please'],\n",
       "   'responses': ['Tell me how can assist you',\n",
       "    'Tell me your problem to assist you',\n",
       "    'Yes Sure, How can I support you']},\n",
       "  {'tag': 'createaccount',\n",
       "   'patterns': ['I need to create a new account',\n",
       "    'how to open a new account',\n",
       "    'I want to create an account',\n",
       "    'can you create an account for me',\n",
       "    'how to open a new account'],\n",
       "   'responses': ['You can just easily create a new account from our web site',\n",
       "    'Just go to our web site and follow the guidelines to create a new account']},\n",
       "  {'tag': 'complaint',\n",
       "   'patterns': ['have a complaint',\n",
       "    'I want to raise a complaint',\n",
       "    'there is a complaint about a service'],\n",
       "   'responses': ['Please provide us your complaint in order to assist you',\n",
       "    'Please mention your complaint, we will reach you and sorry for any inconvenience caused']}]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\"intents\": [\n",
    "    {\"tag\": \"greeting\",\n",
    "     \"patterns\": [\"Hi\", \"Hey\", \"Is anyone there?\", \"Hello\", \"Hay\"],\n",
    "     \"responses\": [\"Hello\", \"Hi\", \"Hi there\"]\n",
    "    },\n",
    "    {\"tag\": \"goodbye\",\n",
    "     \"patterns\": [\"Bye\", \"See you later\", \"Goodbye\"],\n",
    "     \"responses\": [\"See you later\", \"Have a nice day\", \"Bye! Come back again\"]\n",
    "    },\n",
    "    {\"tag\": \"thanks\",\n",
    "     \"patterns\": [\"Thanks\", \"Thank you\", \"That's helpful\", \"Thanks for the help\"],\n",
    "     \"responses\": [\"Happy to help!\", \"Any time!\", \"My pleasure\", \"You're most welcome!\"]\n",
    "    },\n",
    "    {\"tag\": \"about\",\n",
    "     \"patterns\": [\"Who are you?\", \"What are you?\", \"Who you are?\" ],\n",
    "     \"responses\": [\"I.m Joana, your bot assistant\", \"I'm Joana, an Artificial Intelligent bot\"]\n",
    "    },\n",
    "    {\"tag\": \"name\",\n",
    "    \"patterns\": [\"what is your name\", \"what should I call you\", \"whats your name?\"],\n",
    "    \"responses\": [\"You can call me Joana.\", \"I'm Joana!\", \"Just call me as Joana\"]\n",
    "    },\n",
    "    {\"tag\": \"help\",\n",
    "    \"patterns\": [\"Could you help me?\", \"give me a hand please\", \"Can you help?\", \"What can you do for me?\", \"I need a support\", \"I need a help\", \"support me please\"],\n",
    "    \"responses\": [\"Tell me how can assist you\", \"Tell me your problem to assist you\", \"Yes Sure, How can I support you\"]\n",
    "    },\n",
    "    {\"tag\": \"createaccount\",\n",
    "    \"patterns\": [\"I need to create a new account\", \"how to open a new account\", \"I want to create an account\", \"can you create an account for me\", \"how to open a new account\"],\n",
    "    \"responses\": [\"You can just easily create a new account from our web site\", \"Just go to our web site and follow the guidelines to create a new account\"]\n",
    "    },\n",
    "    {\"tag\": \"complaint\",\n",
    "    \"patterns\": [\"have a complaint\", \"I want to raise a complaint\", \"there is a complaint about a service\"],\n",
    "    \"responses\": [\"Please provide us your complaint in order to assist you\", \"Please mention your complaint, we will reach you and sorry for any inconvenience caused\"]\n",
    "    }\n",
    "]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\gaurk\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'intents.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mintents.json\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      2\u001b[0m     data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[0;32m      4\u001b[0m training_sentences \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\gaurk\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'intents.json'"
     ]
    }
   ],
   "source": [
    "with open('intents.json') as file:\n",
    "    data = json.load(file)\n",
    "    \n",
    "training_sentences = []\n",
    "training_labels = []\n",
    "labels = []\n",
    "responses = []\n",
    "\n",
    "\n",
    "for intent in data['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        training_sentences.append(pattern)\n",
    "        training_labels.append(intent['tag'])\n",
    "    responses.append(intent['responses'])\n",
    "    \n",
    "    if intent['tag'] not in labels:\n",
    "        labels.append(intent['tag'])\n",
    "        \n",
    "num_classes = len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl_encoder = LabelEncoder()\n",
    "lbl_encoder.fit(training_labels)\n",
    "training_labels = lbl_encoder.transform(training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1000\n",
    "embedding_dim = 16\n",
    "max_len = 20\n",
    "oov_token = \"<OOV>\"\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_token)\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "padded_sequences = pad_sequences(sequences, truncating='post', maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential()\n\u001b[0;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Embedding(vocab_size, embedding_dim, input_length\u001b[38;5;241m=\u001b[39mmax_len))\n\u001b[0;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39madd(GlobalAveragePooling1D())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Sequential' is not defined"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=max_len))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', \n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "epochs = 500\n",
    "history = model.fit(padded_sequences, np.array(training_labels), epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to save the trained model\n",
    "model.save(\"chat_model\")\n",
    "\n",
    "import pickle\n",
    "\n",
    "# to save the fitted tokenizer\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# to save the fitted label encoder\n",
    "with open('label_encoder.pickle', 'wb') as ecn_file:\n",
    "    pickle.dump(lbl_encoder, ecn_file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import colorama \n",
    "colorama.init()\n",
    "from colorama import Fore, Style, Back\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "with open(\"intents.json\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "\n",
    "def chat():\n",
    "    # load trained model\n",
    "    model = keras.models.load_model('chat_model')\n",
    "\n",
    "    # load tokenizer object\n",
    "    with open('tokenizer.pickle', 'rb') as handle:\n",
    "        tokenizer = pickle.load(handle)\n",
    "\n",
    "    # load label encoder object\n",
    "    with open('label_encoder.pickle', 'rb') as enc:\n",
    "        lbl_encoder = pickle.load(enc)\n",
    "\n",
    "    # parameters\n",
    "    max_len = 20\n",
    "    \n",
    "    while True:\n",
    "        print(Fore.LIGHTBLUE_EX + \"User: \" + Style.RESET_ALL, end=\"\")\n",
    "        inp = input()\n",
    "        if inp.lower() == \"quit\":\n",
    "            break\n",
    "\n",
    "        result = model.predict(keras.preprocessing.sequence.pad_sequences(tokenizer.texts_to_sequences([inp]),\n",
    "                                             truncating='post', maxlen=max_len))\n",
    "        tag = lbl_encoder.inverse_transform([np.argmax(result)])\n",
    "\n",
    "        for i in data['intents']:\n",
    "            if i['tag'] == tag:\n",
    "                print(Fore.GREEN + \"ChatBot:\" + Style.RESET_ALL , np.random.choice(i['responses']))\n",
    "\n",
    "        # print(Fore.GREEN + \"ChatBot:\" + Style.RESET_ALL,random.choice(responses))\n",
    "\n",
    "print(Fore.YELLOW + \"Start messaging with the bot (type quit to stop)!\" + Style.RESET_ALL)\n",
    "chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
